#!/usr/bin/env python3
"""æµ‹è¯•å¼ºåŒ–å­¦ä¹ ç¯å¢ƒ"""

import numpy as np
from shoggoth_mini.training.rl.environment import TentacleTargetFollowingEnv
from shoggoth_mini.configs.rl_training import RLEnvironmentConfig

def test_rl_environment():
    """æµ‹è¯•RLç¯å¢ƒåŸºæœ¬åŠŸèƒ½"""
    
    print("ğŸš€ Creating RL environment...")
    
    # åˆ›å»ºç¯å¢ƒé…ç½®
    config = RLEnvironmentConfig()
    
    # åˆ›å»ºç¯å¢ƒ
    env = TentacleTargetFollowingEnv(config=config, render_mode=None)
    
    print(f"âœ… Environment created successfully!")
    print(f"   - Action space: {env.action_space}")
    print(f"   - Observation space: {env.observation_space}")
    print(f"   - Max episode steps: {env._max_episode_steps}")
    print(f"   - Time per step: {env.time_per_step:.3f}s")
    
    # é‡ç½®ç¯å¢ƒ
    print(f"\nğŸ”„ Resetting environment...")
    obs, info = env.reset()
    print(f"   - Initial observation shape: {obs.shape}")
    print(f"   - Target position: {info.get('target_trajectory', [{}])[0].get('position', 'N/A')}")
    
    # æµ‹è¯•éšæœºåŠ¨ä½œ
    print(f"\nğŸ® Testing random actions...")
    total_reward = 0
    
    for step in range(10):
        # éšæœº2DåŠ¨ä½œï¼ˆå…‰æ ‡ä½ç½®ï¼‰
        action = env.action_space.sample()
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(action)
        total_reward += reward
        
        # è·å–ä¿¡æ¯
        tip_pos = info.get('tip_position', [0, 0, 0])
        target_pos = info.get('target_position', [0, 0, 0])
        distance = info.get('distance', 0)
        
        if step % 3 == 0:
            print(f"   Step {step}:")
            print(f"     - Action (2D cursor): [{action[0]:.3f}, {action[1]:.3f}]")
            print(f"     - Tip position: [{tip_pos[0]:.3f}, {tip_pos[1]:.3f}, {tip_pos[2]:.3f}]")
            print(f"     - Target position: [{target_pos[0]:.3f}, {target_pos[1]:.3f}, {target_pos[2]:.3f}]")
            print(f"     - Distance to target: {distance:.3f}m")
            print(f"     - Reward: {reward:.3f}")
        
        if terminated or truncated:
            print(f"   Episode ended at step {step}")
            break
    
    print(f"\nğŸ“Š Episode summary:")
    print(f"   - Total reward: {total_reward:.3f}")
    print(f"   - Final distance: {info.get('distance', 0):.3f}m")
    
    # æµ‹è¯•å›ºå®šåŠ¨ä½œåºåˆ—
    print(f"\nğŸ¯ Testing fixed action sequence...")
    obs, info = env.reset()
    
    # æµ‹è¯•å››ä¸ªæ–¹å‘çš„ç§»åŠ¨
    test_actions = [
        [0.5, 0.0],    # å³
        [0.0, 0.5],    # ä¸Š  
        [-0.5, 0.0],   # å·¦
        [0.0, -0.5],   # ä¸‹
        [0.0, 0.0],    # ä¸­å¿ƒ
    ]
    
    for i, action in enumerate(test_actions):
        obs, reward, terminated, truncated, info = env.step(np.array(action))
        tip_pos = info.get('tip_position', [0, 0, 0])
        tendon_lengths = info.get('tendon_lengths', [0, 0, 0])
        
        print(f"   Action {i+1} {action}: tip=[{tip_pos[0]:.3f}, {tip_pos[1]:.3f}, {tip_pos[2]:.3f}], "
              f"tendons=[{tendon_lengths[0]:.3f}, {tendon_lengths[1]:.3f}, {tendon_lengths[2]:.3f}]")
    
    env.close()
    print(f"\nğŸ‰ RL environment test completed successfully!")

if __name__ == "__main__":
    test_rl_environment()
