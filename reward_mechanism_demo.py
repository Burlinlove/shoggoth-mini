#!/usr/bin/env python3
"""è¯¦ç»†è§£é‡Šå¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶çš„æ¼”ç¤º"""

import numpy as np
from shoggoth_mini.training.rl.environment import TentacleTargetFollowingEnv
from shoggoth_mini.configs.rl_training import RLEnvironmentConfig
import time

def explain_reward_mechanism():
    """è¯¦ç»†è§£é‡Šå¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶"""
    print("ğŸ¯ å¼ºåŒ–å­¦ä¹ å¥–åŠ±æœºåˆ¶è¯¦è§£")
    print("=" * 50)
    
    print("\nğŸ’¡ æ ¸å¿ƒæ¦‚å¿µ:")
    print("   1. MuJoCoä»¿çœŸ = è™šæ‹Ÿçš„'çœŸå®ç¯å¢ƒ'")
    print("   2. ç‰©ç†ä»¿çœŸæä¾›çŠ¶æ€ä¿¡æ¯")
    print("   3. ç¨‹åºé€»è¾‘è®¡ç®—å¥–åŠ±ä¿¡å·")
    print("   4. å¥–åŠ±å¼•å¯¼æ™ºèƒ½ä½“å­¦ä¹ ")
    
    print("\nğŸ”„ äº¤äº’å¾ªç¯:")
    print("   æ™ºèƒ½ä½“å‘å‡ºåŠ¨ä½œ â†’ MuJoCoä»¿çœŸæ‰§è¡Œ â†’ è·å¾—æ–°çŠ¶æ€ â†’ è®¡ç®—å¥–åŠ± â†’ åé¦ˆç»™æ™ºèƒ½ä½“")

def demonstrate_reward_calculation():
    """å®é™…æ¼”ç¤ºå¥–åŠ±è®¡ç®—è¿‡ç¨‹"""
    print("\nğŸ§ª å¥–åŠ±è®¡ç®—æ¼”ç¤º")
    print("=" * 30)
    
    # åˆ›å»ºç¯å¢ƒ
    config = RLEnvironmentConfig()
    env = TentacleTargetFollowingEnv(config=config, render_mode=None)
    
    print("âœ… ç¯å¢ƒå·²åˆ›å»º")
    
    # é‡ç½®ç¯å¢ƒ
    obs, info = env.reset()
    
    print(f"\nğŸ¬ å¼€å§‹æ¼”ç¤ºå¥–åŠ±è®¡ç®—...")
    print("=" * 40)
    
    # æ‰§è¡Œå‡ ä¸ªåŠ¨ä½œï¼Œè¯¦ç»†å±•ç¤ºå¥–åŠ±è®¡ç®—è¿‡ç¨‹
    test_actions = [
        ([0.5, 0.0], "å‘å³ç§»åŠ¨"),
        ([0.0, 0.5], "å‘ä¸Šç§»åŠ¨"),
        ([-0.5, 0.0], "å‘å·¦ç§»åŠ¨"),
        ([0.0, -0.5], "å‘ä¸‹ç§»åŠ¨"),
        ([0.0, 0.0], "ä¿æŒä¸­å¿ƒ")
    ]
    
    total_reward = 0
    
    for step, (action, description) in enumerate(test_actions):
        print(f"\nğŸ“ æ­¥éª¤ {step + 1}: {description}")
        print("-" * 25)
        
        # æ‰§è¡ŒåŠ¨ä½œå‰çš„çŠ¶æ€
        old_tip_pos = env._get_tip_position()
        target_pos = env.target_position.copy()
        old_distance = np.linalg.norm(old_tip_pos - target_pos)
        
        print(f"   åŠ¨ä½œå‰çŠ¶æ€:")
        print(f"     - è§¦æ‰‹å°–ç«¯: [{old_tip_pos[0]:.3f}, {old_tip_pos[1]:.3f}, {old_tip_pos[2]:.3f}]")
        print(f"     - ç›®æ ‡ä½ç½®: [{target_pos[0]:.3f}, {target_pos[1]:.3f}, {target_pos[2]:.3f}]")
        print(f"     - è·ç¦»: {old_distance:.3f}m")
        
        # æ‰§è¡ŒåŠ¨ä½œ
        obs, reward, terminated, truncated, info = env.step(np.array(action))
        total_reward += reward
        
        # æ‰§è¡ŒåŠ¨ä½œåçš„çŠ¶æ€
        new_tip_pos = info['tip_position']
        new_target_pos = info['target_position']
        new_distance = info['distance']
        
        print(f"\n   ğŸ’» MuJoCoä»¿çœŸæ‰§è¡Œ:")
        print(f"     - è¾“å…¥åŠ¨ä½œ: [{action[0]:.3f}, {action[1]:.3f}] (2Då…‰æ ‡)")
        print(f"     - è½¬æ¢ä¸ºè…±ç»³é•¿åº¦: {info['tendon_lengths']}")
        print(f"     - ç‰©ç†ä»¿çœŸè¿è¡Œ {env.frame_skip} æ­¥")
        
        print(f"\n   ğŸ¯ å¥–åŠ±è®¡ç®—è¯¦è§£:")
        print(f"     - æ–°çš„å°–ç«¯ä½ç½®: [{new_tip_pos[0]:.3f}, {new_tip_pos[1]:.3f}, {new_tip_pos[2]:.3f}]")
        print(f"     - æ–°çš„ç›®æ ‡ä½ç½®: [{new_target_pos[0]:.3f}, {new_target_pos[1]:.3f}, {new_target_pos[2]:.3f}]")
        print(f"     - æ–°çš„è·ç¦»: {new_distance:.3f}m")
        
        # è¯¦ç»†å±•ç¤ºå¥–åŠ±ç»„æˆ
        distance_penalty = info['distance_penalty']
        action_penalty = info.get('action_penalty', 0)
        
        print(f"\n   ğŸ§® å¥–åŠ±ç»„æˆ:")
        print(f"     - è·ç¦»æƒ©ç½š: -{distance_penalty:.3f}")
        print(f"     - åŠ¨ä½œæƒ©ç½š: -{action_penalty:.3f}")
        print(f"     - æ€»å¥–åŠ±: {reward:.3f}")
        
        # åˆ†æå¥–åŠ±å«ä¹‰
        if reward > -0.2:
            feedback = "ğŸ˜Š ä¸é”™! è·ç¦»è¾ƒè¿‘"
        elif reward > -0.5:
            feedback = "ğŸ˜ ä¸€èˆ¬, è·ç¦»ä¸­ç­‰"
        else:
            feedback = "ğŸ˜ è¾ƒå·®, è·ç¦»è¾ƒè¿œ"
        
        print(f"     - åé¦ˆ: {feedback}")
        
        if terminated or truncated:
            print("   ğŸ”š Episodeç»“æŸ")
            break
    
    print(f"\nğŸ“Š æ€»ç»“:")
    print(f"   - æ€»ç´¯ç§¯å¥–åŠ±: {total_reward:.3f}")
    print(f"   - æ‰§è¡Œæ­¥æ•°: {len(test_actions)}")
    print(f"   - å¹³å‡å¥–åŠ±: {total_reward/len(test_actions):.3f}")
    
    env.close()

def explain_reward_sources():
    """è§£é‡Šå¥–åŠ±çš„ä¸åŒæ¥æº"""
    print("\nğŸ—ï¸ MuJoCoä»¿çœŸä¸­çš„'ç¯å¢ƒ'æ„æˆ:")
    print("=" * 40)
    
    print("ğŸ“ 1. ç‰©ç†ä»¿çœŸå™¨ (MuJoCo)")
    print("   - æä¾›çœŸå®çš„ç‰©ç†åé¦ˆ")
    print("   - è®¡ç®—ç¢°æ’ã€é‡åŠ›ã€æ‘©æ“¦ç­‰")
    print("   - æ›´æ–°æœºå™¨äººçŠ¶æ€")
    print("   ä½œç”¨: æ¨¡æ‹ŸçœŸå®ä¸–ç•Œçš„ç‰©ç†è§„å¾‹")
    
    print("\nğŸ¯ 2. ä»»åŠ¡å®šä¹‰ (ç›®æ ‡è®¾å®š)")
    print("   - è®¾å®šç›®æ ‡ä½ç½®")
    print("   - å®šä¹‰æˆåŠŸæ ‡å‡†")
    print("   - åˆ›å»ºè½¨è¿¹åºåˆ—")
    print("   ä½œç”¨: å‘Šè¯‰æ™ºèƒ½ä½“è¦å®Œæˆä»€ä¹ˆä»»åŠ¡")
    
    print("\nğŸ§® 3. å¥–åŠ±å‡½æ•° (ç¨‹åºé€»è¾‘)")
    print("   - æµ‹é‡å½“å‰çŠ¶æ€ä¸ç›®æ ‡çš„å·®è·")
    print("   - è®¡ç®—æ•°å€¼åŒ–çš„å¥–åŠ±ä¿¡å·")
    print("   - æä¾›å­¦ä¹ åé¦ˆ")
    print("   ä½œç”¨: æŒ‡å¯¼æ™ºèƒ½ä½“å­¦ä¹ æ–¹å‘")
    
    print("\nğŸ“Š 4. è§‚å¯Ÿç³»ç»Ÿ (ä¼ æ„Ÿå™¨æ¨¡æ‹Ÿ)")
    print("   - è·å–è§¦æ‰‹å°–ç«¯ä½ç½®")
    print("   - è¯»å–å…³èŠ‚è§’åº¦")
    print("   - æµ‹é‡è…±ç»³é•¿åº¦")
    print("   ä½œç”¨: æä¾›æ™ºèƒ½ä½“æ„ŸçŸ¥ç¯å¢ƒçš„ä¿¡æ¯")

def compare_sim_vs_real():
    """å¯¹æ¯”ä»¿çœŸç¯å¢ƒä¸çœŸå®ç¯å¢ƒ"""
    print("\nğŸ”„ ä»¿çœŸç¯å¢ƒ vs çœŸå®ç¯å¢ƒ")
    print("=" * 40)
    
    print("ğŸ¤– çœŸå®æœºå™¨äººç¯å¢ƒ:")
    print("   äº¤äº’æµç¨‹: åŠ¨ä½œ â†’ ç”µæœºæ§åˆ¶ â†’ ç‰©ç†è¿åŠ¨ â†’ ä¼ æ„Ÿå™¨è¯»å– â†’ å¥–åŠ±è®¡ç®—")
    print("   å¥–åŠ±æ¥æº: ä»»åŠ¡å®Œæˆæƒ…å†µ (å¦‚: ç‰©ä½“åˆ°è¾¾ç›®æ ‡ä½ç½®)")
    print("   åé¦ˆç±»å‹: çœŸå®ä¼ æ„Ÿå™¨æ•°æ®")
    print("   é™åˆ¶å› ç´ : ç¡¬ä»¶å®‰å…¨ã€æ—¶é—´æˆæœ¬ã€ç£¨æŸ")
    
    print("\nğŸ’» MuJoCoä»¿çœŸç¯å¢ƒ:")
    print("   äº¤äº’æµç¨‹: åŠ¨ä½œ â†’ ä»¿çœŸå™¨è®¡ç®— â†’ è™šæ‹Ÿè¿åŠ¨ â†’ çŠ¶æ€æ›´æ–° â†’ å¥–åŠ±è®¡ç®—")
    print("   å¥–åŠ±æ¥æº: ç¼–ç¨‹å®šä¹‰çš„ç›®æ ‡å‡½æ•°")
    print("   åé¦ˆç±»å‹: ä»¿çœŸå™¨è®¡ç®—çš„å‡†ç¡®æ•°æ®")
    print("   ä¼˜åŠ¿å› ç´ : å®‰å…¨å¿«é€Ÿã€å¯é‡å¤ã€æˆæœ¬ä½")
    
    print("\nğŸ¯ æ ¸å¿ƒinsight:")
    print("   æ— è®ºçœŸå®è¿˜æ˜¯ä»¿çœŸï¼Œå¥–åŠ±éƒ½æ¥è‡ªäº'ä»»åŠ¡å®Œæˆç¨‹åº¦çš„é‡åŒ–æµ‹é‡'")
    print("   MuJoCoæä¾›çš„æ˜¯é«˜ä¿çœŸçš„ç‰©ç†ä»¿çœŸï¼Œè®©å­¦åˆ°çš„ç­–ç•¥èƒ½è½¬ç§»åˆ°çœŸå®ç¯å¢ƒ")

def show_code_walkthrough():
    """å±•ç¤ºä»£ç å±‚é¢çš„å¥–åŠ±è®¡ç®—"""
    print("\nğŸ’» ä»£ç å±‚é¢çš„å¥–åŠ±è®¡ç®—")
    print("=" * 30)
    
    print("ğŸ“ åœ¨ environment.py çš„ step() å‡½æ•°ä¸­:")
    print("""
    # 1. æ‰§è¡ŒåŠ¨ä½œ (ä¸ç¯å¢ƒäº¤äº’)
    mujoco.mj_step(self.model, self.data)  # ç‰©ç†ä»¿çœŸè®¡ç®—
    
    # 2. è·å–æ–°çŠ¶æ€
    tip_pos = self._get_tip_position()      # ä»ä»¿çœŸä¸­è¯»å–è§¦æ‰‹ä½ç½®
    target_pos = self.target_position       # å½“å‰ç›®æ ‡ä½ç½®
    distance = np.linalg.norm(tip_pos - target_pos)  # è®¡ç®—è·ç¦»
    
    # 3. è®¡ç®—å¥–åŠ± (è¿™é‡Œæ˜¯äººå·¥è®¾è®¡çš„å¥–åŠ±å‡½æ•°)
    distance_penalty = self.reward_distance_scale * (distance ** exponent)
    action_penalty = self.action_change_penalty_scale * action_magnitude
    reward = -distance_penalty - action_penalty
    
    # 4. è¿”å›ç»™æ™ºèƒ½ä½“
    return observation, reward, terminated, truncated, info
    """)
    
    print("ğŸ¯ å…³é”®ç‚¹:")
    print("   - MuJoCoæä¾›ç‰©ç†çŠ¶æ€ (tip_pos)")
    print("   - ç¨‹åºå®šä¹‰å¥–åŠ±é€»è¾‘ (distance_penalty)")
    print("   - å¥–åŠ±æŒ‡å¯¼å­¦ä¹ æ–¹å‘ (è¶Šé è¿‘ç›®æ ‡å¥–åŠ±è¶Šé«˜)")

if __name__ == "__main__":
    explain_reward_mechanism()
    demonstrate_reward_calculation()
    explain_reward_sources()
    compare_sim_vs_real()
    show_code_walkthrough()
    
    print(f"\nğŸŠ æ€»ç»“:")
    print("   MuJoCoä»¿çœŸç¯å¢ƒå®Œç¾æ¨¡æ‹Ÿäº†çœŸå®ä¸–ç•Œçš„äº¤äº’è¿‡ç¨‹ï¼")
    print("   æ™ºèƒ½ä½“é€šè¿‡æ•°ç™¾ä¸‡æ¬¡è¿™æ ·çš„äº¤äº’å¾ªç¯æ¥å­¦ä¹ æœ€ä¼˜ç­–ç•¥ã€‚")
