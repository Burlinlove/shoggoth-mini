#!/usr/bin/env python3
"""æ·±å…¥å±•ç¤ºPPOå†…éƒ¨å¦‚ä½•è·å–å’Œä½¿ç”¨step()è¿”å›å€¼"""

import numpy as np
from shoggoth_mini.training.rl.environment import TentacleTargetFollowingEnv
from shoggoth_mini.configs.rl_training import RLEnvironmentConfig
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

def simulate_ppo_internal_collection():
    """æ¨¡æ‹ŸPPOå†…éƒ¨çš„æ•°æ®æ”¶é›†æœºåˆ¶"""
    print("ğŸ”§ æ¨¡æ‹ŸPPOå†…éƒ¨æ•°æ®æ”¶é›†æœºåˆ¶")
    print("=" * 50)
    
    # åˆ›å»ºç¯å¢ƒï¼ˆåƒPPOä¸€æ ·ï¼‰
    config = RLEnvironmentConfig()
    
    def make_env():
        return TentacleTargetFollowingEnv(config=config, render_mode=None)
    
    env = DummyVecEnv([make_env for _ in range(2)])
    
    # åˆ›å»ºç®€å•çš„ç­–ç•¥ç½‘ç»œï¼ˆæ¨¡æ‹ŸPPOçš„ç­–ç•¥ï¼‰
    model = PPO("MlpPolicy", env, learning_rate=3e-4, n_steps=5, verbose=0)
    
    print(f"âœ… ç¯å¢ƒå’Œæ¨¡å‹å·²åˆ›å»º")
    print(f"   - ç¯å¢ƒæ•°: 2")
    print(f"   - æ”¶é›†æ­¥æ•°: 5")
    
    print(f"\nğŸ”„ æ¨¡æ‹Ÿcollect_rollouts()å‡½æ•°:")
    print("-" * 40)
    
    # æ‰‹åŠ¨å®ç°collect_rolloutsçš„æ ¸å¿ƒé€»è¾‘
    observations = env.reset()
    print(f"ç¯å¢ƒé‡ç½®: observations.shape = {observations.shape}")
    
    # æ¨¡æ‹Ÿæ”¶é›†5æ­¥æ•°æ®
    collected_data = {
        'observations': [],
        'actions': [],
        'rewards': [],
        'dones': [],
        'values': [],
        'log_probs': []
    }
    
    for step in range(5):
        print(f"\nğŸ“Š æ”¶é›†æ­¥éª¤ {step+1}:")
        
        # 1. ç­–ç•¥ç½‘ç»œé¢„æµ‹ï¼ˆæ¨¡æ‹ŸPPOå†…éƒ¨ï¼‰
        actions, values, log_probs = model.policy(observations)
        
        print(f"   ğŸ§  ç­–ç•¥ç½‘ç»œè¾“å‡º:")
        print(f"     - actions: {actions.detach().numpy()}")
        print(f"     - values: {values.detach().numpy().flatten()}")
        
        # 2. â­ å…³é”®ï¼šç¯å¢ƒstep()è°ƒç”¨
        new_observations, rewards, dones, infos = env.step(actions.detach().numpy())
        
        print(f"   ğŸ“¤ ç¯å¢ƒstep()è¿”å›:")
        print(f"     - observations: {new_observations.shape}")
        print(f"     - rewards: {rewards}")
        print(f"     - dones: {dones}")
        print(f"     - infos: {len(infos)}ä¸ªå­—å…¸")
        
        # 3. æ•°æ®å­˜å‚¨ï¼ˆæ¨¡æ‹Ÿrollout_buffer.add()ï¼‰
        collected_data['observations'].append(observations.copy())
        collected_data['actions'].append(actions.detach().numpy().copy())
        collected_data['rewards'].append(rewards.copy())
        collected_data['dones'].append(dones.copy())
        collected_data['values'].append(values.detach().numpy().copy())
        collected_data['log_probs'].append(log_probs.detach().numpy().copy())
        
        print(f"   ğŸ’¾ æ•°æ®å·²å­˜å‚¨åˆ°ç¼“å†²åŒº")
        
        # 4. æ›´æ–°è§‚å¯Ÿç”¨äºä¸‹ä¸€æ­¥
        observations = new_observations
    
    # åˆ†ææ”¶é›†çš„æ•°æ®
    print(f"\nğŸ“ˆ æ”¶é›†å®Œæˆï¼Œæ•°æ®ç»Ÿè®¡:")
    print(f"   - æ€»æ­¥æ•°: {len(collected_data['observations'])}")
    print(f"   - æ€»æ ·æœ¬æ•°: {len(collected_data['observations']) * collected_data['observations'][0].shape[0]}")
    print(f"   - è§‚å¯Ÿç»´åº¦: {collected_data['observations'][0].shape}")
    print(f"   - åŠ¨ä½œç»´åº¦: {collected_data['actions'][0].shape}")
    print(f"   - å¥–åŠ±èŒƒå›´: [{min(r.min() for r in collected_data['rewards']):.3f}, {max(r.max() for r in collected_data['rewards']):.3f}]")
    
    env.close()
    return collected_data

def explain_rollout_buffer_mechanism():
    """è§£é‡ŠRolloutBufferçš„æœºåˆ¶"""
    print(f"\nğŸ“¦ RolloutBufferæœºåˆ¶è¯¦è§£")
    print("=" * 50)
    
    print(f"ğŸ¯ RolloutBufferçš„ä½œç”¨:")
    print(f"   - ä¸´æ—¶å­˜å‚¨PPOæ”¶é›†çš„ç»éªŒæ•°æ®")
    print(f"   - æ”¯æŒæ‰¹é‡å¤„ç†å’Œå‘é‡åŒ–è®¡ç®—")
    print(f"   - è‡ªåŠ¨è®¡ç®—ä¼˜åŠ¿å‡½æ•°å’Œå›æŠ¥")
    
    print(f"\nğŸ“Š æ•°æ®å­˜å‚¨æ ¼å¼:")
    print(f"   Bufferç»´åº¦: [n_steps, n_envs, ...]")
    print(f"   å®é™…ç¤ºä¾‹: [400, 6, ...]")
    print(f"   ")
    print(f"   observations: [400, 6, 36]  # æ¯æ­¥æ¯ç¯å¢ƒçš„è§‚å¯Ÿ")
    print(f"   actions:      [400, 6, 2]   # æ¯æ­¥æ¯ç¯å¢ƒçš„åŠ¨ä½œ")
    print(f"   rewards:      [400, 6]      # æ¯æ­¥æ¯ç¯å¢ƒçš„å¥–åŠ±")
    print(f"   dones:        [400, 6]      # æ¯æ­¥æ¯ç¯å¢ƒçš„å®Œæˆæ ‡å¿—")
    print(f"   values:       [400, 6]      # ä»·å€¼ç½‘ç»œçš„è¾“å‡º")
    print(f"   log_probs:    [400, 6]      # åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡")
    
    print(f"\nğŸ”„ ä½¿ç”¨æ—¶æœº:")
    print(f"   æ”¶é›†é˜¶æ®µ: rollout_buffer.add() é€æ­¥æ·»åŠ æ•°æ®")
    print(f"   è®­ç»ƒé˜¶æ®µ: rollout_buffer.get() è·å–æ‰¹æ¬¡æ•°æ®")
    print(f"   é‡ç½®é˜¶æ®µ: rollout_buffer.reset() æ¸…ç©ºç¼“å†²åŒº")

def show_ppo_training_cycle():
    """å±•ç¤ºPPOçš„å®Œæ•´è®­ç»ƒå‘¨æœŸ"""
    print(f"\nğŸ”„ PPOå®Œæ•´è®­ç»ƒå‘¨æœŸ")
    print("=" * 50)
    
    print(f"ğŸƒ è®­ç»ƒå¾ªç¯ä¼ªä»£ç :")
    print(f'''
    def learn(total_timesteps):
        for iteration in range(total_timesteps // (n_steps * n_envs)):
            
            # ğŸ—‚ï¸ é˜¶æ®µ1: æ•°æ®æ”¶é›†
            observations = self.env.reset()
            for step in range(n_steps):  # é»˜è®¤400æ­¥
                actions, values, log_probs = self.policy(observations)
                
                # â­ æ ¸å¿ƒï¼šè·å–ç¯å¢ƒè¿”å›å€¼
                new_obs, rewards, dones, infos = self.env.step(actions)
                
                # å­˜å‚¨æ‰€æœ‰è¿”å›å€¼ï¼ˆé™¤äº†infoï¼‰
                self.rollout_buffer.add(
                    obs=observations,      # step()è¿”å›çš„ä¸Šä¸€æ­¥è§‚å¯Ÿ
                    action=actions,        # ç­–ç•¥è¾“å‡ºçš„åŠ¨ä½œ
                    reward=rewards,        # step()è¿”å›çš„å¥–åŠ± â­
                    done=dones,           # step()è¿”å›çš„å®Œæˆæ ‡å¿— â­
                    value=values,         # ä»·å€¼ç½‘ç»œè¾“å‡º
                    log_prob=log_probs    # åŠ¨ä½œæ¦‚ç‡
                )
                
                observations = new_obs  # step()è¿”å›çš„æ–°è§‚å¯Ÿ â­
            
            # ğŸ“ é˜¶æ®µ2: ç­–ç•¥æ›´æ–°
            rollout_data = self.rollout_buffer.get()
            for epoch in range(n_epochs):  # é»˜è®¤5è½®
                self.train_step(rollout_data)
            
            # ğŸ”„ é˜¶æ®µ3: é‡ç½®ç¼“å†²åŒº
            self.rollout_buffer.reset()
    ''')

def explain_key_insights():
    """è§£é‡Šå…³é”®æ´å¯Ÿ"""
    print(f"\nğŸ’¡ å…³é”®æ´å¯Ÿ")
    print("=" * 50)
    
    print(f"ğŸ¯ step()è¿”å›å€¼çš„å®é™…ç”¨é€”:")
    print(f"   observation â†’ PPOç­–ç•¥ç½‘ç»œçš„ä¸‹ä¸€æ­¥è¾“å…¥")
    print(f"   reward â†’ è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ŒæŒ‡å¯¼æ¢¯åº¦æ–¹å‘")
    print(f"   done â†’ æ§åˆ¶episodeè¾¹ç•Œï¼Œé¿å…è·¨episodeå­¦ä¹ ")
    print(f"   info â†’ ä»…ç”¨äºäººç±»è°ƒè¯•ï¼ŒPPOç®—æ³•å®Œå…¨å¿½ç•¥ï¼")
    
    print(f"\nğŸ§  PPOä¸ºä»€ä¹ˆä¸éœ€è¦infoä¸­çš„è·ç¦»ï¼Ÿ")
    print(f"   1. PPOæ˜¯ç«¯åˆ°ç«¯å­¦ä¹ : observation â†’ action")
    print(f"   2. ä¸­é—´è®¡ç®—è¿‡ç¨‹å¯¹PPOé€æ˜")
    print(f"   3. rewardå·²ç»åŒ…å«äº†è·ç¦»çš„è¯„ä»·ä¿¡æ¯")
    print(f"   4. è¿™ç§è®¾è®¡è®©PPOæ›´é€šç”¨ï¼ˆä¸ä¾èµ–ç‰¹å®šåŸŸçŸ¥è¯†ï¼‰")
    
    print(f"\nâš¡ é«˜æ•ˆå¤„ç†çš„å…³é”®:")
    print(f"   - å‘é‡åŒ–: åŒæ—¶å¤„ç†å¤šä¸ªç¯å¢ƒçš„è¿”å›å€¼")
    print(f"   - æ‰¹å¤„ç†: æ”¶é›†å¤šæ­¥æ•°æ®åä¸€èµ·è®­ç»ƒ")
    print(f"   - å†…å­˜å¤ç”¨: RolloutBufferé«˜æ•ˆç®¡ç†æ•°æ®")
    
    print(f"\nğŸ” ä»£ç è°ƒç”¨é“¾:")
    print(f"   model.learn() â†’ collect_rollouts() â†’ env.step() â†’ environment.py step()")
    print(f"   â†‘ ç”¨æˆ·è°ƒç”¨    â†‘ PPOå†…éƒ¨        â†‘ å‘é‡åŒ–å¤„ç†   â†‘ MuJoCoä»¿çœŸ")

def demonstrate_buffer_data_usage():
    """æ¼”ç¤ºç¼“å†²åŒºæ•°æ®çš„ä½¿ç”¨"""
    print(f"\nğŸ“Š ç¼“å†²åŒºæ•°æ®ä½¿ç”¨æ¼”ç¤º")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæ”¶é›†çš„æ•°æ®
    n_steps, n_envs = 5, 2
    obs_dim, action_dim = 36, 2
    
    # æ¨¡æ‹Ÿæ•°æ®ï¼ˆä¸ä¸Šé¢æ”¶é›†çš„æ•°æ®å¯¹åº”ï¼‰
    fake_observations = np.random.randn(n_steps, n_envs, obs_dim)
    fake_rewards = np.random.uniform(-0.5, -0.1, (n_steps, n_envs))
    fake_values = np.random.uniform(-0.3, 0, (n_steps, n_envs))
    
    print(f"ğŸ“¦ æ¨¡æ‹Ÿç¼“å†²åŒºæ•°æ®:")
    print(f"   - observations: {fake_observations.shape}")
    print(f"   - rewards: {fake_rewards.shape}")
    print(f"   - values: {fake_values.shape}")
    
    # è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼ˆPPOçš„æ ¸å¿ƒï¼‰
    advantages = fake_rewards - fake_values
    
    print(f"\nğŸ§® ä¼˜åŠ¿å‡½æ•°è®¡ç®—:")
    print(f"   advantages = rewards - values")
    print(f"   å½¢çŠ¶: {advantages.shape}")
    print(f"   ä½œç”¨: å‘Šè¯‰PPOå“ªäº›åŠ¨ä½œæ¯”é¢„æœŸå¥½/å")
    
    print(f"\nğŸ“ˆ æ‰¹æ¬¡è®­ç»ƒ:")
    print(f"   - å°† {n_steps}Ã—{n_envs}={n_steps*n_envs} ä¸ªæ ·æœ¬ç»„æˆæ‰¹æ¬¡")
    print(f"   - æ¯ä¸ªæ ·æœ¬: (observation, action, advantage)")
    print(f"   - è®­ç»ƒç­–ç•¥ç½‘ç»œè¾“å‡ºæ›´å¥½çš„åŠ¨ä½œåˆ†å¸ƒ")

if __name__ == "__main__":
    # æ¼”ç¤ºæ•°æ®æ”¶é›†
    collected_data = simulate_ppo_internal_collection()
    
    # è§£é‡Šbufferæœºåˆ¶
    explain_rollout_buffer_mechanism()
    
    # å±•ç¤ºè®­ç»ƒå‘¨æœŸ
    show_ppo_training_cycle()
    
    # è§£é‡Šå…³é”®æ´å¯Ÿ
    explain_key_insights()
    
    # æ¼”ç¤ºæ•°æ®ä½¿ç”¨
    demonstrate_buffer_data_usage()
    
    print(f"\nğŸŠ æœ€ç»ˆå›ç­”:")
    print(f"   PPOé€šè¿‡æ ‡å‡†çš„env.step()è°ƒç”¨è·å–è¿”å›å€¼ï¼Œ")
    print(f"   å°†observation/reward/doneå­˜å‚¨åˆ°RolloutBufferï¼Œ")
    print(f"   ç„¶åæ‰¹é‡ä½¿ç”¨è¿™äº›æ•°æ®æ›´æ–°ç­–ç•¥ç½‘ç»œï¼")
    print(f"   infoå­—å…¸ä¸­çš„è·ç¦»ç­‰ä¿¡æ¯ä»…ç”¨äºäººç±»è°ƒè¯•ï¼ŒPPOå®Œå…¨ä¸ä½¿ç”¨ï¼")
