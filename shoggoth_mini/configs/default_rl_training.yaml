rl_training:
  rl_env:
    # MuJoCo simulation settings
    xml_file: "assets/simulation/tentacle.xml"
    simulation_length_seconds: 8.0
    time_between_steps_seconds: 0.08

    # Actuator/tendon settings
    initial_actuator_position: [0.23, 0.23]

    # Reward shaping
    reward_distance_scale: 1.0
    distance_penalty_exponent: 1.0
    action_change_penalty_scale: 0.25

    # Environment objects
    tip_site_name: "tip_center"
    target_bounds_min: [-0.2, -0.15, 0.12]
    target_bounds_max: [-0.1, 0.15, 0.30]

    # Observation settings
    num_frames: 4
    include_actuator_lengths_in_obs: true

    # Action space
    max_2d_action_magnitude: 1.0

    # Trajectory generation
    pause_probability: 0.2
    min_pause_duration: 0.5
    max_pause_duration: 3.0
    min_move_duration: 0.5
    max_move_duration: 2.0

    # Sim2Real domain randomization
    randomize_dynamics: true
    randomization_factors:
      body_mass: 0.05
      body_inertia: 0.05
      dof_damping: 0.05
      jnt_stiffness: 0.05
      geom_friction: 0.05
      geom_solref: 0.05
      geom_solimp: 0.05
      actuator_gainprm: 0.0
      actuator_biasprm: 0.0

    # Observation noise
    add_observation_noise: true
    observation_noise_scale: 0.005

    # Fixed target (optional)
    fixed_target_position: null

  rl_training_params:
    # Environment setup
    num_envs: 6

    # Training duration
    total_timesteps: 1000000000

    # Evaluation
    eval_freq: 10000
    n_eval_episodes: 5

    # Checkpointing
    save_freq: 50000
    log_dir_base: "results"

    # PPO hyperparameters
    learning_rate: 0.0003
    n_steps: 400
    batch_size: 64
    n_epochs: 5
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    ent_coef: 0.0

    # Network architecture
    net_arch: "256-256"
    activation_fn: "Tanh"

  rl_evaluation:
    num_episodes: 10
    render_delay: 0.05
    deterministic_actions: true
    render_mode: null 